{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODyfCb0vcgr0XNgVT1Zbud",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zuleka061/Data-structure/blob/main/SVN_And_Navie_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM AND NAIVE BAYES ASSIGNMENT"
      ],
      "metadata": {
        "id": "J4iblMqI4_Zz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Support vector Machine (SVM ) , and how does it work ?\n",
        "\n",
        "--> A Support Vector Machine (SVM) is a machine learning algorithm used for classification and regression tasks .It works  by finding the best hyperplane ( a plane or line ) that separates the data into different classes . The goal is to maximize the margin between the classes making it easier to classify new data points ."
      ],
      "metadata": {
        "id": "4IRIQuqg5JCM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.  Explain the difference between Hard Maargin and Soft MArgin SVM ?\n",
        "\n",
        "--> Hard Margin SVM : this type of SVM requires that the data be linarly separable , it can be separated by a straight line or plane without any errors . It doesn't allows for misclassification .\n",
        "\n",
        "Soft Margin SVM : this type off SVM allows for some misclassification by introducing slack variables . It's used when the data isn't linearly seprable. this is used in real world data with errors."
      ],
      "metadata": {
        "id": "3L7YnXDe6QU-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the Kernel Tricks in SVM ? give one example of a Kernel and explain its use case ?\n",
        "\n",
        "--> The Kernel trick is a technique used in SVM to transform the original data into a higher dimension space , making it easier to separate the classes .\n",
        "\n",
        "Example : Radial Basic Function (RBF) kernel\n",
        "          The RBF kernel is a popular kernel used in SVM. It's useful when the data linearly separable in the original space . The RBF kernel maps teh data to a higher - dimensional space , allowing for non=linear separation ."
      ],
      "metadata": {
        "id": "WK9CH4-97eY_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is Naive Bayes Classifier , and why is it called \"Naive \"?\n",
        "\n",
        "--> A Naive Bayes Classifier is a machin learning algorithm used for classifiacation tasks . it's based on Bayes's theorem and assumes that the features are independent to each other .\n",
        "\n",
        "    The \"Naive \" part refers to this assumption , which is often not true in real world data ."
      ],
      "metadata": {
        "id": "deJwRhvr9ohc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.  Describe the Gaussian Multinomial , and Bernoulli Naive Bayes variants . When would you use each one ?\n",
        "\n",
        "--> 1, Gaussian Naive Bayes : this variant assumes that the features follow a Gaussian (normal ) distribution\n",
        "\n",
        "  Use Case : when your features are continuous and can be modeled using a normal distribution\n",
        "\n",
        "  Ex : heigh , weight\n",
        "\n",
        "  2. Multinomial Naive Bayes : this variant is suitable for features that represent counts or frequencies . It assumes that the data follow a multinomial distribution .\n",
        "\n",
        "  Use Case : use multinomial Naive Bayes for text classification tasks where features are word counts .\n",
        "\n",
        "  3. Bernoulli Naive Bayes : this variant is used when the features are binary (0,1) . It assumes that the data follows a multivariate Bernoulli distribution .\n",
        "\n",
        "  Use Case :  Use Bernoulli Naive Bayes when your features are binary.\n",
        "\n",
        "  Ex. in text classification where features indicte the presence or absence of a word ."
      ],
      "metadata": {
        "id": "l98VFVF_-0Y9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to\n",
        "\n",
        "Load the Iris dataset\n",
        "\n",
        "Train an SVM Classifier with a linear kernel\n",
        "\n",
        "Print the model's accuracy and support vectors ."
      ],
      "metadata": {
        "id": "o5ukxdnAHCqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm\n",
        "from sklearn import metrics\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "classifier = svm.SVC(kernel='linear')\n",
        "classifier.fit(X_train, y_train)\n",
        "y_pred = classifier.predict(X_test)\n",
        "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Support Vectors:\")\n",
        "print(classifier.support_vectors_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w62s5MlPHaqo",
        "outputId": "d573f56e-fc18-414f-80ea-abb8a7577c58"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Support Vectors:\n",
            "[[4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.7 3.  5.  1.7]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [5.9 3.  5.1 1.8]\n",
            " [4.9 2.5 4.5 1.7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to\n",
        "\n",
        "Load the breast cancer dataset\n",
        "\n",
        "Train a Gaussian Naive Bayes model\n",
        "\n",
        "Print its classification report including precision , recall, and F1- score"
      ],
      "metadata": {
        "id": "FaQt0-NJI2fU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "x = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "classifier = GaussianNB()\n",
        "classifier.fit(x_train, y_train)\n",
        "y_pred = classifier.predict(x_test)\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SE9QYrYjJT5t",
        "outputId": "2311c279-f930-43c6-dbb6-1ec85e7051ed"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.93      0.96        43\n",
            "           1       0.96      1.00      0.98        71\n",
            "\n",
            "    accuracy                           0.97       114\n",
            "   macro avg       0.98      0.97      0.97       114\n",
            "weighted avg       0.97      0.97      0.97       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python Program to\n",
        "\n",
        "Train an SVM Classifier on the wine dataset using GridSearchCV to find the best C and gamma .\n",
        "\n",
        "Print the best hyperparameters andaccuracy ."
      ],
      "metadata": {
        "id": "xwR5sIV5J4R-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001]}\n",
        "grid_search = GridSearchCV(svm.SVC(), param_grid, cv=5)\n",
        "grid_search.fit(x_train, y_train)\n",
        "best_params = grid_search.best_params_\n",
        "best_accuracy = grid_search.best_score_\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"Best Accuracy:\", best_accuracy)\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(x_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuHZhnYsKkPb",
        "outputId": "654ad35c-7d6a-46f0-e2fa-5f33d62643b2"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 100, 'gamma': 0.001}\n",
            "Best Accuracy: 0.7179802955665024\n",
            "Test Accuracy: 0.8333333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to\n",
        "\n",
        "Train a Naive Bayes Classifier on a synthesis text dataset (e.g using sklearn.datasets ,fetch-20newsgroups ).\n",
        "\n",
        "print the model's ROC-AUC score for its predictions ."
      ],
      "metadata": {
        "id": "DgtLFrpQLf1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the 20 Newsgroups dataset\n",
        "newsgroups_data = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
        "X = newsgroups_data.data\n",
        "y = newsgroups_data.target\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "vectorizer = CountVectorizer()\n",
        "x_train_vectorized = vectorizer.fit_transform(x_train)\n",
        "x_test_vectorized = vectorizer.transform(x_test)\n",
        "classifier = MultinomialNB()\n",
        "classifier.fit(x_train_vectorized, y_train)\n",
        "y_pred_proba = classifier.predict_proba(x_test_vectorized)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
        "print(\"ROC-AUC Score:\", roc_auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9BVPMe7MTOg",
        "outputId": "83decb03-757c-43c7-b8ce-66c53197e701"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9097379109453442\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Imagine you're working as a data scientist for a company that handles email communications .\n",
        "\n",
        "your task is to automatically classify emails as spam or not spam . the emails may contain .\n",
        "\n",
        "\n",
        "\n",
        "*   text with diverse vocabulary\n",
        "*   potential class imbalance\n",
        "*   Some incomplete or missing data .\n",
        "\n",
        "\n",
        "\n",
        "* preprocess the data\n",
        "*  Chose and justify an appropriate model\n",
        "* Address class imbalance\n",
        "* Evaluate the performance of your solution with suitable metrics and explain the business impact of your solution .\n",
        "\n"
      ],
      "metadata": {
        "id": "qNU6QLXYM8ZI"
      }
    }
  ]
}