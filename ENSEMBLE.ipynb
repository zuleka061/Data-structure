{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZ8wehtS13QThEqSoXMW1u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zuleka061/Data-structure/blob/main/ENSEMBLE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**bold text**"
      ],
      "metadata": {
        "id": "VvGcXtl2Jo5N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ENSEMBLE LEARNING"
      ],
      "metadata": {
        "id": "gumShf6EJq76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is ensemble learning in machine learning ? Explain the key idea behind it ?\n",
        "\n",
        "--> Ensemble learning is combining multiple models to make better prediction.\n",
        "\n",
        "The key idea behind the Ensemble learning is ⁉\n",
        "\n",
        "1. multiple models improve the overall performance .\n",
        "\n",
        "2. reduce overfitting .\n",
        "\n",
        "3. increase robustness."
      ],
      "metadata": {
        "id": "WBecaMHjJ3m3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between BAGGING AND BOOSTING ?\n",
        "\n",
        "--> Bagging :Trains models separately and combines them to reduce errors .\n",
        "\n",
        "Boosting :Trains models one after another , each fixing mistakes of the previous one ."
      ],
      "metadata": {
        "id": "a3BETastLAT2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is bootstrap sampling and what role does it play in Bagging methods in Random Forest ?\n",
        "\n",
        "--> Bootstrap sampling : Creating subsets of data by sampling with replacement .\n",
        "\n",
        "Role in Bagging : helps create different subsets for training multiple model, reducing overfitting ,"
      ],
      "metadata": {
        "id": "Xx1r7G81Lw3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What are Out Of Bag(OOB) samples and how is OOB score used to evaluate ensemble models ?\n",
        "\n",
        "--> OOB samples : Data points not selected in a bootstrap sample for a particular tree .\n",
        "\n",
        "OOB score : used to estimate model performance without needing a separate test set ."
      ],
      "metadata": {
        "id": "uHYQPEmuMj1P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Compare features importance analysis in a single Decision Tree vs a Random Forest ?\n",
        "\n",
        "-->Decision Tree : features importance can be biased towards features used higher in the tree.\n",
        "\n",
        "Random Forest : gives a more robust and reliable measure of feature importance by averaging across many trees ."
      ],
      "metadata": {
        "id": "3wtuqCphNTJz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python prpgram to\n",
        "Load the Breast cancer dataset using\n",
        "\n",
        "sklearn. datasets.load_breast_cancer()\n",
        "\n",
        "Train a Random Forest Classifier\n",
        "\n",
        "Print the top 5 most important features based on features importance scores ."
      ],
      "metadata": {
        "id": "-1ZAKrFiOdyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "data = load_breast_cancer()\n",
        "x = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target)\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "rf.fit(x, y)\n",
        "importances = pd.Series(rf.feature_importances_, index=x.columns)\n",
        "print(importances.sort_values(ascending=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpepWFUdPPgF",
        "outputId": "f95edda5-7360-4d0d-df43-1f497be048cc"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "worst area                 0.132556\n",
            "worst concave points       0.103334\n",
            "mean concave points        0.095819\n",
            "worst perimeter            0.082368\n",
            "worst radius               0.082029\n",
            "mean radius                0.066146\n",
            "mean area                  0.060246\n",
            "mean concavity             0.055753\n",
            "worst concavity            0.055596\n",
            "mean perimeter             0.051062\n",
            "area error                 0.033530\n",
            "worst texture              0.027042\n",
            "worst compactness          0.023244\n",
            "perimeter error            0.017135\n",
            "radius error               0.014410\n",
            "mean texture               0.013751\n",
            "worst smoothness           0.013098\n",
            "concavity error            0.010162\n",
            "worst symmetry             0.010146\n",
            "worst fractal dimension    0.007746\n",
            "mean smoothness            0.006015\n",
            "mean compactness           0.005611\n",
            "smoothness error           0.005027\n",
            "mean symmetry              0.004985\n",
            "compactness error          0.004531\n",
            "concave points error       0.004191\n",
            "symmetry error             0.003989\n",
            "fractal dimension error    0.003982\n",
            "texture error              0.003690\n",
            "mean fractal dimension     0.002808\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to\n",
        "\n",
        "Train a Bagging Classifier using decission trees on Iris dataset.\n",
        "\n",
        "Evaluate its accuracy and compare with a single decission tree"
      ],
      "metadata": {
        "id": "DA4HwytwPnO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "x, y = data.data, data.target\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "dt = DecisionTreeClassifier()\n",
        "dt.fit(x_train, y_train)\n",
        "dt_pred = dt.predict(x_test)\n",
        "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, dt_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIxJouGUQN5H",
        "outputId": "a49a14f6-c546-458c-925d-2fd8d7eacfa1"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to ⁉\n",
        "\n",
        "Train a Random Forest Classifier\n",
        "\n",
        "Tune hyperparameters max_depth and n_estimation using GridSearchCV\n",
        "\n",
        "Print the best parameter and final accuracy ."
      ],
      "metadata": {
        "id": "PGwEnFhEQgxk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from  sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "data = load_iris()\n",
        "x, y = data.data, data.target\n",
        "\n",
        "rf = RandomForestClassifier\n",
        "\n",
        "param_grid = {'max_depth': [3, 5, 7], 'n_estimators': [50, 100, 150]}\n",
        "\n",
        "grid_search = GridSearchCV(rf(), param_grid, cv=5)\n",
        "grid_search.fit(x, y)\n",
        "\n",
        "print(grid_search.best_params_)\n",
        "print(grid_search.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlppdKPlRC20",
        "outputId": "56d4203b-0642-4ccd-caf9-e87d05e08a59"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'max_depth': 5, 'n_estimators': 150}\n",
            "0.9666666666666668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to ⁉\n",
        "\n",
        "Train a Bagging Regressor and a Random Forest Regressor on the California housing dataset .\n",
        "\n",
        "Compare the Mean squared Errors (MSE)"
      ],
      "metadata": {
        "id": "6hj1HunlReYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "california = fetch_california_housing()\n",
        "x, y = california.data, california.target\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n",
        "\n",
        "bagging = BaggingRegressor()\n",
        "bagging.fit(x_train, y_train)\n",
        "bagging_pred = bagging.predict(x_test)\n",
        "print(\"Bagging MSE:\", mean_squared_error(y_test, bagging_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYzoB8StR6PU",
        "outputId": "4b0ffbad-a880-446d-aec6-2c52b84d53f9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging MSE: 0.28591211250447496\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.  You are working as a data scientist at a finacial institution to predict loan default . You have access to customer demograohic and transcation history data .\n",
        "\n",
        "--> STEPS\n",
        "\n",
        "1. CHOOSE BETWEEN BAGGING AND BOOSTING : Bagging (Bootstrap Aggregrating ) reduces variance by training models on different subsets of data .\n",
        "\n",
        "Boosting reduces bias by sequentially training models to correct previous errors . For loan default prediction with potentially imbalanced data , Boosting (like XGBOOST ) might perform well.\n",
        "\n",
        "2. Handle overfitting : use technique like limiting tree depth , early stopping or regularization to prevent overfitting .\n",
        "\n",
        "3. Select Base Models : decission trees are common base models for Bagging ( Random Forest ) or Boosting (XGBOOST )\n",
        "\n",
        "4. Evaluate performance using cross validation : use k-fold cross validation to acess model perfomance on unseen data , considering metrics like AUC - ROC for imbalanced classes .\n",
        "\n",
        "5. Justify how ensemble learning improves decision-making : Ensemble methods improve accuracy and robustness by combining multiple models . In loan default prediction ,  this leads to better risk assessment reducing potential losses for the financial intituition ."
      ],
      "metadata": {
        "id": "JQYpSVguSoCA"
      }
    }
  ]
}