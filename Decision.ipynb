{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8H5ypKlGi3+1rkLrrR0aH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zuleka061/Data-structure/blob/main/Decision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DECISION TREE"
      ],
      "metadata": {
        "id": "eoXrb-1OEvj4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a Decision Tree , and how does it work in the context of classification ?\n",
        "\n",
        "--> A Decision Tree is a supervised learning algorithm used for both classification and regression tasks.\n",
        "\n",
        "  It works by recursively partitioning the data into smaller subset based on the feature of the input data. In the context of classification , the tree is constructed by selecting the best feature to split the data at each node , with the goal of creating subsets that are as pure as possible in trems of the target variable."
      ],
      "metadata": {
        "id": "hcM4wcTzE2SI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Explain the concept of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decission Tree ?\n",
        "\n",
        "--> Gini impurity : Measures the proability of incorrectly classifying a randomly chosen elemnts in the dataset if it were randomly labeled according to the class distribution of the parents node . Lower Gini impurity indicates a pirer node.\n",
        "\n",
        "Entropy : Measures the amount of uncertainty or randomness in the data . Lower entropy indicates a more ordered or pure node.\n",
        "\n",
        "Both measures are used to determine the best split at each node . The split that results in the lowest impurity (Gini or Entropy is chosen )."
      ],
      "metadata": {
        "id": "DPc20EjzFKnP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the difference between Pre=Pruning and Post-Pruning in Decission Tree ?Give one pratical advantage of using each ?\n",
        "\n",
        "--> Pre-Pruning : Involes stopping the growth of the tree early , before it is fully grown , based on certain criteria such as max depth or min samples perleaf .\n",
        "\n",
        "Practical advantage : Reduces computational cost and prevents overfitting.\n",
        "\n",
        "Post-pruning : Involves growing the tree to its full depth and then removing branches that do not contribute significantly to the model's accuracy .\n",
        "\n",
        "Advantage : can lead to more accurate models as it considered the entire tree structure before pruning ."
      ],
      "metadata": {
        "id": "_BH1tYE-Fl_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is information Gain in Decision Trees, and why is it important for choosing the best split ?\n",
        "\n",
        "--> Information Gain measures the reduction in impurity or uncertaintyafter plitting the data based on particular feartures . It is calculated as the difference between the impurity of the parent node and the weighted average of the impurities of the child nodes .\n",
        "\n",
        "    The features with the highest information gain is chosen for splitting because it results in the most significsnt reduction in impurity , leading to a more accurate classification ."
      ],
      "metadata": {
        "id": "6TrHjrS-GGTP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are some common real-world application of Decision Trees, and what are their main advantage and limitations ?\n",
        "\n",
        "--> REAL WORLD APPLICATIONS\n",
        "      1. Credit Risk Assessment .\n",
        "      2. Medical Diagnosis .\n",
        "      3. Customer Segmentation .\n",
        "\n",
        "    ADVANTAGES      \n",
        "        1. Interpretability .\n",
        "        2. Handling Missing Values .\n",
        "        3. Non-Parametric .\n",
        "\n",
        "    LIMITATIONS\n",
        "        1. Overfitting .\n",
        "        2. Instability .\n",
        "        3. Greedy Algorithm ."
      ],
      "metadata": {
        "id": "uJNh4rdgGgG5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a python program to\n",
        "\n",
        "  Load the iris dataset\n",
        "\n",
        "\n",
        "  Train a Decision Tree Classifier using the Gini criterion\n",
        "\n",
        "  \n",
        "  Print the model's accuracy and feature importances ."
      ],
      "metadata": {
        "id": "Of6kNZoHMtzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "clf = DecisionTreeClassifier(criterion='gini' )\n",
        "clf.fit(x_train, y_train)\n",
        "y_pred = clf.predict(x_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "feature_importances = clf.feature_importances_\n",
        "for feature, importance in zip(iris.feature_names, feature_importances):\n",
        "    print(f\"{feature}: {importance}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulQSo-h1Pbxq",
        "outputId": "b5e2a8ee-159a-45e7-d1cf-5a96d3338903"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Model Accuracy: 100.00%\n",
            "sepal length (cm): 0.016670139612419255\n",
            "sepal width (cm): 0.0\n",
            "petal length (cm): 0.9061433868879218\n",
            "petal width (cm): 0.07718647349965893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python Program to\n",
        "\n",
        "    Load the iris Dataset\n",
        "\n",
        "    Train a DEcision Tree Classifier using the Gini criterion\n",
        "    \n",
        "     \n",
        "    Print the model's accuracy and features importances"
      ],
      "metadata": {
        "id": "elQH64j1Ni1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "clf_depth_3 = DecisionTreeClassifier(criterion='gini', max_depth=3)\n",
        "clf_depth_3.fit(x_train, y_train)\n",
        "clf_full = DecisionTreeClassifier(criterion='gini')\n",
        "clf_full.fit(x_train, y_train)\n",
        "y_pred_depth_3 = clf_depth_3.predict(x_test)\n",
        "y_pred_full = clf_full.predict(x_test)\n",
        "accuracy_depth_3 = accuracy_score(y_test, y_pred_depth_3)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "print(\"Accuracy (max_depth=3):\", accuracy_depth_3)\n",
        "print(\"Accuracy (full tree):\", accuracy_full)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZhCCSLnSobb",
        "outputId": "d96a314d-8d1d-4705-92b5-4b45f5b75eda"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (max_depth=3): 1.0\n",
            "Accuracy (full tree): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to\n",
        "\n",
        "  Load the California housing dataset from sklearn\n",
        "\n",
        "  Train a Decision Tree Regressor\n",
        "\n",
        "  Print the Mean Squared Error (MSE ) and features importances"
      ],
      "metadata": {
        "id": "J8INpntPTWey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "housing = fetch_california_housing()\n",
        "x = housing.data\n",
        "y = housing.target\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "regressor.fit(x_train, y_train)\n",
        "y_pred = regressor.predict(x_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"Mean Squared Error:\", mse)\n",
        "feature_importances = regressor.feature_importances_\n",
        "for feature, importance in zip(housing.feature_names, feature_importances):\n",
        "    print(f\"{feature}: {importance}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_elkeLiUFdZ",
        "outputId": "2a66d995-c5b4-4e87-c590-551e85b242d7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.495235205629094\n",
            "MedInc: 0.5285090936963706\n",
            "HouseAge: 0.05188353710616045\n",
            "AveRooms: 0.05297496833123543\n",
            "AveBedrms: 0.02866045788296106\n",
            "Population: 0.030515676373806224\n",
            "AveOccup: 0.13083767753210346\n",
            "Latitude: 0.09371656401749287\n",
            "Longitude: 0.08290202505986989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python Program\n",
        "\n",
        "Load the iris Dataset\n",
        "\n",
        "Ture the Decision Tree's max_depth and min_samples_split using GridSearchCv\n",
        "\n",
        "Print the best parameters and the resulting model accuracy ."
      ],
      "metadata": {
        "id": "WyX4j_6cVzNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "iris = load_iris()\n",
        "x = iris.data\n",
        "y = iris.target\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5],\n",
        "    'min_samples_split': [2, 3, 4, 5]\n",
        "}\n",
        "clf = DecisionTreeClassifier()\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=5)\n",
        "grid_search.fit(x_train, y_train)\n",
        "best_params = grid_search.best_params_\n",
        "best_accuracy = grid_search.best_score_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Accuracy:\", best_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSzhOlW0WPu5",
        "outputId": "e2746f77-cfe6-4a3f-c74e-11c0cb4b0d4d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n",
            "Best Accuracy: 0.9416666666666668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Imagine you are working as a data scientist for a health care compant that wants to predict whether a patient has a certain disesase . you have a large dataset with mixed data types and some missing values .\n",
        "\n",
        "-->1. Handling Missing values\n",
        "\n",
        "    * Identify the type of missing values ( e.g numerical , categrocial )\n",
        "\n",
        "    * For numerical features , impute missing values using mean , median , or interpolation .\n",
        "\n",
        "    *  For categorical features , impute missing values using mode or a specific category .\n",
        "\n",
        "  2. Encode categorical Features\n",
        "\n",
        "    * Use techniques like One-Hot Encodeing (OHE ) , label encoding or Ordinial encoding .\n",
        "\n",
        "    * OHE is suitable for categorical features with a small number of unique valuees .\n",
        "\n",
        "  3. Train a Decision Tree Model\n",
        "\n",
        "    * Use a Decision Tree classifier from a library like scikit-learn.\n",
        "\n",
        "    * Train the model on the preprocessed data .\n",
        "\n",
        "  4. Tune hyperparameters\n",
        "\n",
        "    * Use techniques like GridSearchCV or RandomizedSearchCV to tune hyperparameters.\n",
        "\n",
        "    * Tune parameters like max_depth , min_sample_split and criterion .\n",
        "\n",
        "  5. Evaluate Performance\n",
        "\n",
        "    * Use metric like accuracy , precision , recall, F1- score , and ROC-AUC.\n",
        "\n",
        "    * Evaluate the model's performance on a hold-out test set."
      ],
      "metadata": {
        "id": "tmyGwTVbWqgI"
      }
    }
  ]
}